# Environment Definition File for Apertus Fine-tuning on Clariden
# This EDF uses the NGC PyTorch container with optimizations for multi-node training

image = "/capstor/store/cscs/ethz/large-sc-2/environment/ngc_pt_jan.sqsh"

# Mount necessary filesystems
mounts = [
    "/capstor",
    "/iopsstor",
    "/users"
]

# Set working directory to the project root (will use current directory)
# workdir is set dynamically by the submit scripts

writable = true

[annotations]
# Enable AWS OFI NCCL plugin for Slingshot interconnect (critical for multi-node)
com.hooks.aws_ofi_nccl.enabled = "true"
com.hooks.aws_ofi_nccl.variant = "cuda12"

[env]
# NCCL Configuration (for distributed training)
NCCL_DEBUG = "INFO"
NCCL_NET = "AWS Libfabric"
NCCL_NET_GDR_LEVEL = "PHB"
NCCL_CROSS_NIC = "1"
NCCL_PROTO = "^LL128"

# PyTorch Configuration
TORCH_NCCL_ASYNC_ERROR_HANDLING = "1"
TRITON_HOME = "/dev/shm/"

# CUDA Configuration
CUDA_CACHE_DISABLE = "1"

# MPICH Configuration
MPICH_GPU_SUPPORT_ENABLED = "0"

# Libfabric Configuration (optimized for Slingshot)
FI_CXI_DEFAULT_CQ_SIZE = "131072"
FI_CXI_DEFAULT_TX_SIZE = "16384"
FI_CXI_DISABLE_HOST_REGISTER = "1"
FI_CXI_RX_MATCH_MODE = "software"
FI_MR_CACHE_MONITOR = "userfaultfd"

# Weights & Biases
WANDB_API_KEY = "ADD KEY"
